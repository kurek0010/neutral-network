{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kurek0010/neutral-network/blob/main/02_basics/07_nn_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmSZRhSCUOB-"
      },
      "source": [
        "* @author: krakowiakpawel9@gmail.com  \n",
        "* @site: e-smartdata.org"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsUWcfin6ZlT"
      },
      "source": [
        "### Implementacja prostej sieci neuronowej\n",
        "\n",
        "##### Kroki:\n",
        "    1. Zainicjowanie parametrów sieci\n",
        "    2. Propagacja wprzód\n",
        "    3. Obliczenie błędu predykcji\n",
        "    4. Propagacja wsteczna (uczenie modelu)\n",
        "    5. Test działania modelu\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkDZb9Nt04rg"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "X = np.array([1.4, 0.7])\n",
        "y_true = np.array([1.8])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zc5rkn_m1IjB"
      },
      "source": [
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "    W1 = np.random.rand(n_h, n_x)\n",
        "    W2 = np.random.rand(n_h, n_y)\n",
        "    return W1, W2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G43XdqDa1exJ"
      },
      "source": [
        "def forward_propagation(X, W1, W2):\n",
        "    H1 = np.dot(X, W1)\n",
        "    y_pred = np.dot(H1, W2)\n",
        "    return H1, y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbaAd2jt1v8t"
      },
      "source": [
        "def calculate_error(y_pred, y_true):\n",
        "    return y_pred - y_true"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3QFBEn_14Lj"
      },
      "source": [
        "def predict(X, W1, W2):\n",
        "    _, y_pred = forward_propagation(X, W1, W2)\n",
        "    return y_pred[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rClKyNy02CnX"
      },
      "source": [
        "def backpropagation(X, W1, W2, learning_rate, iters=1000, precision=0.0000001):\n",
        "\n",
        "    H1, y_pred = forward_propagation(X, W1, W2)\n",
        "    train_loss = []\n",
        "\n",
        "    for i in range(iters):\n",
        "        error = calculate_error(y_pred, y_true)\n",
        "        W2 = W2 - learning_rate * error * H1.T\n",
        "        W1 = W1 - learning_rate * error * np.dot(X.T, W2.T)\n",
        "\n",
        "        y_pred = predict(X, W1, W2)\n",
        "        print(f'Iter #{i}: y_pred {y_pred}: loss: {abs(calculate_error(y_pred, y_true[0]))}')\n",
        "        train_loss.append(abs(calculate_error(y_pred, y_true[0])))\n",
        "\n",
        "        if abs(error) < precision:\n",
        "            break\n",
        "\n",
        "    return W1, W2, train_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phVhArP520f3"
      },
      "source": [
        "def build_model():\n",
        "\n",
        "    W1, W2 = initialize_parameters(2, 2, 1)\n",
        "\n",
        "    W1, W2, train_loss = backpropagation(X, W1, W2, 0.01)\n",
        "\n",
        "    model = {'W1': W1, 'W2': W2, 'train_loss': train_loss}\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg7M4xDv3JFT",
        "outputId": "19f5d852-3b8d-4c06-b7a6-b4dc35c10e54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = build_model()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter #0: y_pred 0.47775082418068876: loss: 1.3222491758193113\n",
            "Iter #1: y_pred 0.5203197164995874: loss: 1.2796802835004126\n",
            "Iter #2: y_pred 0.5624603087798719: loss: 1.2375396912201282\n",
            "Iter #3: y_pred 0.6041630898849911: loss: 1.195836910115009\n",
            "Iter #4: y_pred 0.6454096471221384: loss: 1.1545903528778616\n",
            "Iter #5: y_pred 0.686174266816854: loss: 1.113825733183146\n",
            "Iter #6: y_pred 0.7264254411736586: loss: 1.0735745588263415\n",
            "Iter #7: y_pred 0.7661272789328608: loss: 1.0338727210671392\n",
            "Iter #8: y_pred 0.8052408159236343: loss: 0.9947591840763658\n",
            "Iter #9: y_pred 0.8437252210745323: loss: 0.9562747789254677\n",
            "Iter #10: y_pred 0.8815388936200005: loss: 0.9184611063799996\n",
            "Iter #11: y_pred 0.9186404479839603: loss: 0.8813595520160398\n",
            "Iter #12: y_pred 0.9549895839833127: loss: 0.8450104160166874\n",
            "Iter #13: y_pred 0.9905478414374655: loss: 0.8094521585625345\n",
            "Iter #14: y_pred 1.025279239868215: loss: 0.7747207601317851\n",
            "Iter #15: y_pred 1.0591508056150682: loss: 0.7408491943849318\n",
            "Iter #16: y_pred 1.0921329902786125: loss: 0.7078670097213875\n",
            "Iter #17: y_pred 1.1241999858610763: loss: 0.6758000141389238\n",
            "Iter #18: y_pred 1.1553299432399835: loss: 0.6446700567600165\n",
            "Iter #19: y_pred 1.185505101647381: loss: 0.6144948983526191\n",
            "Iter #20: y_pred 1.2147118376107076: loss: 0.5852881623892925\n",
            "Iter #21: y_pred 1.2429406423347058: loss: 0.5570593576652942\n",
            "Iter #22: y_pred 1.2701860367729518: loss: 0.5298139632270482\n",
            "Iter #23: y_pred 1.296446433669521: loss: 0.5035535663304791\n",
            "Iter #24: y_pred 1.3217239556708602: loss: 0.47827604432913984\n",
            "Iter #25: y_pred 1.346024218245523: loss: 0.45397578175447695\n",
            "Iter #26: y_pred 1.3693560856383367: loss: 0.4306439143616634\n",
            "Iter #27: y_pred 1.3917314074600646: loss: 0.4082685925399354\n",
            "Iter #28: y_pred 1.4131647428070897: loss: 0.38683525719291034\n",
            "Iter #29: y_pred 1.4336730780491806: loss: 0.3663269219508194\n",
            "Iter #30: y_pred 1.4532755436450078: loss: 0.34672445635499227\n",
            "Iter #31: y_pred 1.4719931345688555: loss: 0.32800686543114455\n",
            "Iter #32: y_pred 1.489848438177975: loss: 0.31015156182202497\n",
            "Iter #33: y_pred 1.5068653726340866: loss: 0.29313462736591345\n",
            "Iter #34: y_pred 1.5230689383266065: loss: 0.27693106167339354\n",
            "Iter #35: y_pred 1.5384849841373773: loss: 0.2615150158626227\n",
            "Iter #36: y_pred 1.5531399898420248: loss: 0.2468600101579752\n",
            "Iter #37: y_pred 1.567060865463699: loss: 0.232939134536301\n",
            "Iter #38: y_pred 1.5802747679806677: loss: 0.21972523201933236\n",
            "Iter #39: y_pred 1.5928089354380996: loss: 0.2071910645619004\n",
            "Iter #40: y_pred 1.6046905382229744: loss: 0.19530946177702568\n",
            "Iter #41: y_pred 1.6159465470249987: loss: 0.1840534529750013\n",
            "Iter #42: y_pred 1.6266036168207822: loss: 0.1733963831792178\n",
            "Iter #43: y_pred 1.63668798607784: loss: 0.16331201392216\n",
            "Iter #44: y_pred 1.6462253902739443: loss: 0.15377460972605572\n",
            "Iter #45: y_pred 1.6552409887604627: loss: 0.14475901123953738\n",
            "Iter #46: y_pred 1.6637593039605378: loss: 0.1362406960394622\n",
            "Iter #47: y_pred 1.6718041718794512: loss: 0.12819582812054886\n",
            "Iter #48: y_pred 1.6793987029108668: loss: 0.12060129708913325\n",
            "Iter #49: y_pred 1.6865652519449181: loss: 0.11343474805508191\n",
            "Iter #50: y_pred 1.6933253968187492: loss: 0.10667460318125088\n",
            "Iter #51: y_pred 1.6996999241940762: loss: 0.10030007580592382\n",
            "Iter #52: y_pred 1.7057088219969534: loss: 0.09429117800304665\n",
            "Iter #53: y_pred 1.7113712776099892: loss: 0.08862872239001085\n",
            "Iter #54: y_pred 1.7167056810648456: loss: 0.08329431893515449\n",
            "Iter #55: y_pred 1.7217296325414804: loss: 0.07827036745851967\n",
            "Iter #56: y_pred 1.7264599535389993: loss: 0.07354004646100076\n",
            "Iter #57: y_pred 1.7309127011401986: loss: 0.06908729885980147\n",
            "Iter #58: y_pred 1.7351031848471579: loss: 0.06489681515284218\n",
            "Iter #59: y_pred 1.7390459855180398: loss: 0.06095401448196025\n",
            "Iter #60: y_pred 1.7427549759851644: loss: 0.057245024014835666\n",
            "Iter #61: y_pred 1.7462433429812245: loss: 0.05375665701877552\n",
            "Iter #62: y_pred 1.7495236100440095: loss: 0.050476389955990575\n",
            "Iter #63: y_pred 1.7526076611102193: loss: 0.04739233888978078\n",
            "Iter #64: y_pred 1.7555067645458284: loss: 0.04449323545417161\n",
            "Iter #65: y_pred 1.7582315973941327: loss: 0.04176840260586734\n",
            "Iter #66: y_pred 1.7607922696531622: loss: 0.03920773034683789\n",
            "Iter #67: y_pred 1.7631983484217237: loss: 0.036801651578276307\n",
            "Iter #68: y_pred 1.7654588817781245: loss: 0.034541118221875555\n",
            "Iter #69: y_pred 1.7675824222777847: loss: 0.03241757772221532\n",
            "Iter #70: y_pred 1.7695770499756733: loss: 0.03042295002432671\n",
            "Iter #71: y_pred 1.771450394896963: loss: 0.028549605103036946\n",
            "Iter #72: y_pred 1.7732096588947255: loss: 0.026790341105274562\n",
            "Iter #73: y_pred 1.774861636846979: loss: 0.025138363153021093\n",
            "Iter #74: y_pred 1.7764127371572318: loss: 0.023587262842768242\n",
            "Iter #75: y_pred 1.7778690015329075: loss: 0.022130998467092544\n",
            "Iter #76: y_pred 1.7792361240248984: loss: 0.020763875975101653\n",
            "Iter #77: y_pred 1.7805194693191018: loss: 0.019480530680898278\n",
            "Iter #78: y_pred 1.7817240902772846: loss: 0.018275909722715422\n",
            "Iter #79: y_pred 1.7828547447301206: loss: 0.017145255269879422\n",
            "Iter #80: y_pred 1.7839159115298464: loss: 0.016084088470153635\n",
            "Iter #81: y_pred 1.784911805873826: loss: 0.015088194126174015\n",
            "Iter #82: y_pred 1.785846393913453: loss: 0.014153606086547033\n",
            "Iter #83: y_pred 1.7867234066653805: loss: 0.013276593334619502\n",
            "Iter #84: y_pred 1.7875463532440663: loss: 0.012453646755933745\n",
            "Iter #85: y_pred 1.7883185334362186: loss: 0.01168146656378144\n",
            "Iter #86: y_pred 1.7890430496388665: loss: 0.010956950361133533\n",
            "Iter #87: y_pred 1.7897228181836415: loss: 0.010277181816358505\n",
            "Iter #88: y_pred 1.7903605800703621: loss: 0.009639419929637905\n",
            "Iter #89: y_pred 1.7909589111333402: loss: 0.009041088866659885\n",
            "Iter #90: y_pred 1.7915202316638739: loss: 0.008479768336126181\n",
            "Iter #91: y_pred 1.7920468155123246: loss: 0.007953184487675458\n",
            "Iter #92: y_pred 1.7925407986929023: loss: 0.0074592013070977625\n",
            "Iter #93: y_pred 1.7930041875139482: loss: 0.006995812486051856\n",
            "Iter #94: y_pred 1.79343886625601: loss: 0.006561133743990144\n",
            "Iter #95: y_pred 1.7938466044194779: loss: 0.006153395580522192\n",
            "Iter #96: y_pred 1.7942290635629394: loss: 0.005770936437060614\n",
            "Iter #97: y_pred 1.7945878037527445: loss: 0.00541219624725553\n",
            "Iter #98: y_pred 1.7949242896435964: loss: 0.00507571035640364\n",
            "Iter #99: y_pred 1.7952398962092577: loss: 0.0047601037907423205\n",
            "Iter #100: y_pred 1.7955359141417324: loss: 0.00446408585826763\n",
            "Iter #101: y_pred 1.795813554936549: loss: 0.004186445063451005\n",
            "Iter #102: y_pred 1.7960739556810235: loss: 0.003926044318976585\n",
            "Iter #103: y_pred 1.7963181835616495: loss: 0.003681816438350527\n",
            "Iter #104: y_pred 1.7965472401060394: loss: 0.003452759893960655\n",
            "Iter #105: y_pred 1.796762065174116: loss: 0.00323793482588397\n",
            "Iter #106: y_pred 1.7969635407125617: loss: 0.0030364592874383423\n",
            "Iter #107: y_pred 1.7971524942858603: loss: 0.0028475057141397198\n",
            "Iter #108: y_pred 1.7973297023965797: loss: 0.002670297603420302\n",
            "Iter #109: y_pred 1.7974958936069383: loss: 0.0025041063930617558\n",
            "Iter #110: y_pred 1.7976517514730472: loss: 0.002348248526952812\n",
            "Iter #111: y_pred 1.7977979173026548: loss: 0.002202082697345231\n",
            "Iter #112: y_pred 1.7979349927466213: loss: 0.00206500725337877\n",
            "Iter #113: y_pred 1.7980635422338285: loss: 0.0019364577661715732\n",
            "Iter #114: y_pred 1.798184095258689: loss: 0.001815904741311103\n",
            "Iter #115: y_pred 1.7982971485299188: loss: 0.0017028514700812014\n",
            "Iter #116: y_pred 1.7984031679887722: loss: 0.0015968320112278445\n",
            "Iter #117: y_pred 1.798502590704461: loss: 0.0014974092955390983\n",
            "Iter #118: y_pred 1.7985958266540703: loss: 0.0014041733459297934\n",
            "Iter #119: y_pred 1.7986832603938423: loss: 0.0013167396061577463\n",
            "Iter #120: y_pred 1.7987652526283422: loss: 0.0012347473716578516\n",
            "Iter #121: y_pred 1.7988421416836093: loss: 0.001157858316390703\n",
            "Iter #122: y_pred 1.7989142448900772: loss: 0.0010857551099228147\n",
            "Iter #123: y_pred 1.7989818598806964: loss: 0.0010181401193036788\n",
            "Iter #124: y_pred 1.7990452658093747: loss: 0.000954734190625306\n",
            "Iter #125: y_pred 1.7991047244945615: loss: 0.000895275505438553\n",
            "Iter #126: y_pred 1.799160481492506: loss: 0.0008395185074940859\n",
            "Iter #127: y_pred 1.7992127671044686: loss: 0.0007872328955313979\n",
            "Iter #128: y_pred 1.7992617973218947: loss: 0.0007382026781053153\n",
            "Iter #129: y_pred 1.799307774713335: loss: 0.0006922252866650158\n",
            "Iter #130: y_pred 1.799350889256658: loss: 0.0006491107433419518\n",
            "Iter #131: y_pred 1.7993913191199102: loss: 0.0006086808800898069\n",
            "Iter #132: y_pred 1.7994292313939488: loss: 0.0005707686060512085\n",
            "Iter #133: y_pred 1.7994647827798116: loss: 0.0005352172201884553\n",
            "Iter #134: y_pred 1.7994981202335922: loss: 0.000501879766407809\n",
            "Iter #135: y_pred 1.799529381571432: loss: 0.00047061842856810365\n",
            "Iter #136: y_pred 1.799558696037076: loss: 0.0004413039629240778\n",
            "Iter #137: y_pred 1.7995861848342956: loss: 0.00041381516570448973\n",
            "Iter #138: y_pred 1.7996119616263373: loss: 0.00038803837366274685\n",
            "Iter #139: y_pred 1.7996361330044326: loss: 0.0003638669955674523\n",
            "Iter #140: y_pred 1.7996587989272692: loss: 0.00034120107273083455\n",
            "Iter #141: y_pred 1.7996800531332222: loss: 0.0003199468667778316\n",
            "Iter #142: y_pred 1.799699983527016: loss: 0.00030001647298405487\n",
            "Iter #143: y_pred 1.7997186725424064: loss: 0.00028132745759368305\n",
            "Iter #144: y_pred 1.799736197482355: loss: 0.00026380251764512863\n",
            "Iter #145: y_pred 1.7997526308380936: loss: 0.0002473691619064855\n",
            "Iter #146: y_pred 1.7997680405883811: loss: 0.00023195941161890943\n",
            "Iter #147: y_pred 1.799782490480178: loss: 0.00021750951982202338\n",
            "Iter #148: y_pred 1.7997960402918907: loss: 0.00020395970810938024\n",
            "Iter #149: y_pred 1.7998087460802608: loss: 0.00019125391973928707\n",
            "Iter #150: y_pred 1.799820660411923: loss: 0.00017933958807714312\n",
            "Iter #151: y_pred 1.7998318325805622: loss: 0.00016816741943781466\n",
            "Iter #152: y_pred 1.7998423088105868: loss: 0.0001576911894132227\n",
            "Iter #153: y_pred 1.7998521324481316: loss: 0.00014786755186846356\n",
            "Iter #154: y_pred 1.7998613441401952: loss: 0.0001386558598048815\n",
            "Iter #155: y_pred 1.7998699820026307: loss: 0.00013001799736933606\n",
            "Iter #156: y_pred 1.7998780817776976: loss: 0.00012191822230245286\n",
            "Iter #157: y_pred 1.7998856769818121: loss: 0.0001143230181879229\n",
            "Iter #158: y_pred 1.799892799044106: loss: 0.00010720095589400458\n",
            "Iter #159: y_pred 1.799899477436369: loss: 0.00010052256363102252\n",
            "Iter #160: y_pred 1.7999057397949083: loss: 9.426020509173405e-05\n",
            "Iter #161: y_pred 1.7999116120348222: loss: 8.838796517784964e-05\n",
            "Iter #162: y_pred 1.7999171184571692: loss: 8.288154283087046e-05\n",
            "Iter #163: y_pred 1.799922281849463: loss: 7.77181505371427e-05\n",
            "Iter #164: y_pred 1.7999271235799168: loss: 7.287642008324546e-05\n",
            "Iter #165: y_pred 1.7999316636858218: loss: 6.833631417824115e-05\n",
            "Iter #166: y_pred 1.7999359209564247: loss: 6.40790435753047e-05\n",
            "Iter #167: y_pred 1.7999399130106464: loss: 6.0086989353669296e-05\n",
            "Iter #168: y_pred 1.7999436563699636: loss: 5.6343630036481684e-05\n",
            "Iter #169: y_pred 1.7999471665267532: loss: 5.2833473246804985e-05\n",
            "Iter #170: y_pred 1.7999504580083818: loss: 4.954199161821826e-05\n",
            "Iter #171: y_pred 1.7999535444373063: loss: 4.645556269378126e-05\n",
            "Iter #172: y_pred 1.7999564385874307: loss: 4.356141256933732e-05\n",
            "Iter #173: y_pred 1.7999591524369567: loss: 4.084756304334469e-05\n",
            "Iter #174: y_pred 1.7999616972179413: loss: 3.830278205874116e-05\n",
            "Iter #175: y_pred 1.7999640834627697: loss: 3.5916537230340495e-05\n",
            "Iter #176: y_pred 1.7999663210477341: loss: 3.367895226591422e-05\n",
            "Iter #177: y_pred 1.7999684192338994: loss: 3.158076610065841e-05\n",
            "Iter #178: y_pred 1.7999703867054242: loss: 2.9613294575847604e-05\n",
            "Iter #179: y_pred 1.7999722316054934: loss: 2.7768394506688665e-05\n",
            "Iter #180: y_pred 1.7999739615700154: loss: 2.603842998460948e-05\n",
            "Iter #181: y_pred 1.7999755837592204: loss: 2.4416240779645548e-05\n",
            "Iter #182: y_pred 1.7999771048872897: loss: 2.289511271036382e-05\n",
            "Iter #183: y_pred 1.7999785312501402: loss: 2.1468749859865355e-05\n",
            "Iter #184: y_pred 1.7999798687514794: loss: 2.013124852062731e-05\n",
            "Iter #185: y_pred 1.7999811229272353: loss: 1.88770727647114e-05\n",
            "Iter #186: y_pred 1.799982298968469: loss: 1.7701031530981126e-05\n",
            "Iter #187: y_pred 1.7999834017428533: loss: 1.6598257146727136e-05\n",
            "Iter #188: y_pred 1.7999844358148207: loss: 1.5564185179339773e-05\n",
            "Iter #189: y_pred 1.7999854054644528: loss: 1.4594535547196585e-05\n",
            "Iter #190: y_pred 1.799986314705194: loss: 1.368529480605396e-05\n",
            "Iter #191: y_pred 1.799987167300459: loss: 1.283269954099886e-05\n",
            "Iter #192: y_pred 1.799987966779212: loss: 1.2033220788021382e-05\n",
            "Iter #193: y_pred 1.799988716450569: loss: 1.1283549431029272e-05\n",
            "Iter #194: y_pred 1.7999894194174964: loss: 1.0580582503694203e-05\n",
            "Iter #195: y_pred 1.7999900785896479: loss: 9.921410352164983e-06\n",
            "Iter #196: y_pred 1.7999906966954144: loss: 9.303304585595029e-06\n",
            "Iter #197: y_pred 1.79999127629321: loss: 8.723706790059182e-06\n",
            "Iter #198: y_pred 1.799991819782063: loss: 8.180217937026057e-06\n",
            "Iter #199: y_pred 1.799992329411547: loss: 7.670588453079219e-06\n",
            "Iter #200: y_pred 1.7999928072910878: loss: 7.192708912251433e-06\n",
            "Iter #201: y_pred 1.7999932553986981: loss: 6.744601301900133e-06\n",
            "Iter #202: y_pred 1.799993675589161: loss: 6.324410839031458e-06\n",
            "Iter #203: y_pred 1.799994069601709: loss: 5.93039829110964e-06\n",
            "Iter #204: y_pred 1.799994439067222: loss: 5.560932778037042e-06\n",
            "Iter #205: y_pred 1.7999947855149756: loss: 5.214485024440663e-06\n",
            "Iter #206: y_pred 1.7999951103789753: loss: 4.889621024739554e-06\n",
            "Iter #207: y_pred 1.7999954150038868: loss: 4.584996113221607e-06\n",
            "Iter #208: y_pred 1.799995700650605: loss: 4.2993493949428085e-06\n",
            "Iter #209: y_pred 1.799995968501471: loss: 4.0314985290113015e-06\n",
            "Iter #210: y_pred 1.7999962196651662: loss: 3.7803348338361076e-06\n",
            "Iter #211: y_pred 1.7999964551813026: loss: 3.5448186974651463e-06\n",
            "Iter #212: y_pred 1.7999966760247244: loss: 3.3239752756930585e-06\n",
            "Iter #213: y_pred 1.799996883109542: loss: 3.116890457954824e-06\n",
            "Iter #214: y_pred 1.7999970772929192: loss: 2.922707080799114e-06\n",
            "Iter #215: y_pred 1.7999972593786162: loss: 2.740621383834352e-06\n",
            "Iter #216: y_pred 1.7999974301203214: loss: 2.5698796786155498e-06\n",
            "Iter #217: y_pred 1.799997590224767: loss: 2.4097752331364575e-06\n",
            "Iter #218: y_pred 1.799997740354656: loss: 2.2596453439494013e-06\n",
            "Iter #219: y_pred 1.7999978811314055: loss: 2.1188685945805474e-06\n",
            "Iter #220: y_pred 1.7999980131377178: loss: 1.9868622822549753e-06\n",
            "Iter #221: y_pred 1.7999981369199936: loss: 1.8630800064922681e-06\n",
            "Iter #222: y_pred 1.799998252990592: loss: 1.747009408026301e-06\n",
            "Iter #223: y_pred 1.799998361829952: loss: 1.6381700480572192e-06\n",
            "Iter #224: y_pred 1.7999984638885822: loss: 1.5361114178435997e-06\n",
            "Iter #225: y_pred 1.7999985595889243: loss: 1.4404110757482158e-06\n",
            "Iter #226: y_pred 1.799998649327101: loss: 1.3506728990808625e-06\n",
            "Iter #227: y_pred 1.7999987334745566: loss: 1.2665254434107709e-06\n",
            "Iter #228: y_pred 1.7999988123795947: loss: 1.187620405351808e-06\n",
            "Iter #229: y_pred 1.79999888636882: loss: 1.1136311799386789e-06\n",
            "Iter #230: y_pred 1.7999989557484894: loss: 1.0442515105957284e-06\n",
            "Iter #231: y_pred 1.799999020805779: loss: 9.791942210402027e-07\n",
            "Iter #232: y_pred 1.7999990818099745: loss: 9.181900255672559e-07\n",
            "Iter #233: y_pred 1.7999991390135857: loss: 8.609864143860335e-07\n",
            "Iter #234: y_pred 1.7999991926533894: loss: 8.073466106761629e-07\n",
            "Iter #235: y_pred 1.799999242951413: loss: 7.570485871521981e-07\n",
            "Iter #236: y_pred 1.7999992901158497: loss: 7.098841503516695e-07\n",
            "Iter #237: y_pred 1.7999993343419238: loss: 6.656580762154363e-07\n",
            "Iter #238: y_pred 1.7999993758126962: loss: 6.241873038437262e-07\n",
            "Iter #239: y_pred 1.7999994146998226: loss: 5.853001774358546e-07\n",
            "Iter #240: y_pred 1.7999994511642656: loss: 5.488357344152206e-07\n",
            "Iter #241: y_pred 1.7999994853569592: loss: 5.146430408498048e-07\n",
            "Iter #242: y_pred 1.799999517419434: loss: 4.825805659525173e-07\n",
            "Iter #243: y_pred 1.7999995474844042: loss: 4.5251559588344037e-07\n",
            "Iter #244: y_pred 1.7999995756763145: loss: 4.243236855216992e-07\n",
            "Iter #245: y_pred 1.7999996021118574: loss: 3.978881426558445e-07\n",
            "Iter #246: y_pred 1.7999996269004548: loss: 3.7309954525888145e-07\n",
            "Iter #247: y_pred 1.7999996501447126: loss: 3.4985528740705263e-07\n",
            "Iter #248: y_pred 1.7999996719408435: loss: 3.280591565069102e-07\n",
            "Iter #249: y_pred 1.7999996923790667: loss: 3.0762093339298247e-07\n",
            "Iter #250: y_pred 1.7999997115439799: loss: 2.88456020181016e-07\n",
            "Iter #251: y_pred 1.7999997295149113: loss: 2.704850887713661e-07\n",
            "Iter #252: y_pred 1.7999997463662463: loss: 2.536337537772937e-07\n",
            "Iter #253: y_pred 1.7999997621677362: loss: 2.378322638829644e-07\n",
            "Iter #254: y_pred 1.7999997769847864: loss: 2.2301521362955157e-07\n",
            "Iter #255: y_pred 1.7999997908787284: loss: 2.091212716326396e-07\n",
            "Iter #256: y_pred 1.7999998039070721: loss: 1.9609292789546373e-07\n",
            "Iter #257: y_pred 1.7999998161237443: loss: 1.8387625577709343e-07\n",
            "Iter #258: y_pred 1.7999998275793128: loss: 1.724206872832923e-07\n",
            "Iter #259: y_pred 1.7999998383211948: loss: 1.6167880523276779e-07\n",
            "Iter #260: y_pred 1.7999998483938526: loss: 1.5160614741382972e-07\n",
            "Iter #261: y_pred 1.7999998578389798: loss: 1.421610202889667e-07\n",
            "Iter #262: y_pred 1.7999998666956718: loss: 1.33304328242545e-07\n",
            "Iter #263: y_pred 1.7999998750005881: loss: 1.249994119323361e-07\n",
            "Iter #264: y_pred 1.7999998827881043: loss: 1.1721189574487312e-07\n",
            "Iter #265: y_pred 1.7999998900904552: loss: 1.0990954479872528e-07\n",
            "Iter #266: y_pred 1.7999998969378668: loss: 1.0306213327204716e-07\n",
            "Iter #267: y_pred 1.7999999033586818: loss: 9.664131828124312e-08\n",
            "Iter #268: y_pred 1.7999999093794767: loss: 9.062052330754966e-08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR5WBYto4NV2",
        "outputId": "993c5bc2-eadb-410a-8a8f-c01ed2d4a13a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "loss = pd.DataFrame({'train_loss': model['train_loss']})\n",
        "loss = loss.reset_index().rename(columns={'index': 'iter'})\n",
        "loss['iter'] += 1\n",
        "loss.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>iter</th>\n",
              "      <th>train_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1.322249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1.279680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1.237540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1.195837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1.154590</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   iter  train_loss\n",
              "0     1    1.322249\n",
              "1     2    1.279680\n",
              "2     3    1.237540\n",
              "3     4    1.195837\n",
              "4     5    1.154590"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UV2RRs_5H7v",
        "outputId": "c0912c24-8673-44e4-860a-e26712bf5fe8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(data=go.Scatter(x=loss['iter'], y=loss['train_loss'], mode='markers+lines'))\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"c7791f19-5bb8-4182-9e1f-10a2a3690051\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"c7791f19-5bb8-4182-9e1f-10a2a3690051\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'c7791f19-5bb8-4182-9e1f-10a2a3690051',\n",
              "                        [{\"mode\": \"markers+lines\", \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269], \"y\": [1.3222491758193113, 1.2796802835004126, 1.2375396912201282, 1.195836910115009, 1.1545903528778616, 1.113825733183146, 1.0735745588263415, 1.0338727210671392, 0.9947591840763658, 0.9562747789254677, 0.9184611063799996, 0.8813595520160398, 0.8450104160166874, 0.8094521585625345, 0.7747207601317851, 0.7408491943849318, 0.7078670097213875, 0.6758000141389238, 0.6446700567600165, 0.6144948983526191, 0.5852881623892925, 0.5570593576652942, 0.5298139632270482, 0.5035535663304791, 0.47827604432913984, 0.45397578175447695, 0.4306439143616634, 0.4082685925399354, 0.38683525719291034, 0.3663269219508194, 0.34672445635499227, 0.32800686543114455, 0.31015156182202497, 0.29313462736591345, 0.27693106167339354, 0.2615150158626227, 0.2468600101579752, 0.232939134536301, 0.21972523201933236, 0.2071910645619004, 0.19530946177702568, 0.1840534529750013, 0.1733963831792178, 0.16331201392216, 0.15377460972605572, 0.14475901123953738, 0.1362406960394622, 0.12819582812054886, 0.12060129708913325, 0.11343474805508191, 0.10667460318125088, 0.10030007580592382, 0.09429117800304665, 0.08862872239001085, 0.08329431893515449, 0.07827036745851967, 0.07354004646100076, 0.06908729885980147, 0.06489681515284218, 0.06095401448196025, 0.057245024014835666, 0.05375665701877552, 0.050476389955990575, 0.04739233888978078, 0.04449323545417161, 0.04176840260586734, 0.03920773034683789, 0.036801651578276307, 0.034541118221875555, 0.03241757772221532, 0.03042295002432671, 0.028549605103036946, 0.026790341105274562, 0.025138363153021093, 0.023587262842768242, 0.022130998467092544, 0.020763875975101653, 0.019480530680898278, 0.018275909722715422, 0.017145255269879422, 0.016084088470153635, 0.015088194126174015, 0.014153606086547033, 0.013276593334619502, 0.012453646755933745, 0.01168146656378144, 0.010956950361133533, 0.010277181816358505, 0.009639419929637905, 0.009041088866659885, 0.008479768336126181, 0.007953184487675458, 0.0074592013070977625, 0.006995812486051856, 0.006561133743990144, 0.006153395580522192, 0.005770936437060614, 0.00541219624725553, 0.00507571035640364, 0.0047601037907423205, 0.00446408585826763, 0.004186445063451005, 0.003926044318976585, 0.003681816438350527, 0.003452759893960655, 0.00323793482588397, 0.0030364592874383423, 0.0028475057141397198, 0.002670297603420302, 0.0025041063930617558, 0.002348248526952812, 0.002202082697345231, 0.00206500725337877, 0.0019364577661715732, 0.001815904741311103, 0.0017028514700812014, 0.0015968320112278445, 0.0014974092955390983, 0.0014041733459297934, 0.0013167396061577463, 0.0012347473716578516, 0.001157858316390703, 0.0010857551099228147, 0.0010181401193036788, 0.000954734190625306, 0.000895275505438553, 0.0008395185074940859, 0.0007872328955313979, 0.0007382026781053153, 0.0006922252866650158, 0.0006491107433419518, 0.0006086808800898069, 0.0005707686060512085, 0.0005352172201884553, 0.000501879766407809, 0.00047061842856810365, 0.0004413039629240778, 0.00041381516570448973, 0.00038803837366274685, 0.0003638669955674523, 0.00034120107273083455, 0.0003199468667778316, 0.00030001647298405487, 0.00028132745759368305, 0.00026380251764512863, 0.0002473691619064855, 0.00023195941161890943, 0.00021750951982202338, 0.00020395970810938024, 0.00019125391973928707, 0.00017933958807714312, 0.00016816741943781466, 0.0001576911894132227, 0.00014786755186846356, 0.0001386558598048815, 0.00013001799736933606, 0.00012191822230245286, 0.0001143230181879229, 0.00010720095589400458, 0.00010052256363102252, 9.426020509173405e-05, 8.838796517784964e-05, 8.288154283087046e-05, 7.77181505371427e-05, 7.287642008324546e-05, 6.833631417824115e-05, 6.40790435753047e-05, 6.0086989353669296e-05, 5.6343630036481684e-05, 5.2833473246804985e-05, 4.954199161821826e-05, 4.645556269378126e-05, 4.356141256933732e-05, 4.084756304334469e-05, 3.830278205874116e-05, 3.5916537230340495e-05, 3.367895226591422e-05, 3.158076610065841e-05, 2.9613294575847604e-05, 2.7768394506688665e-05, 2.603842998460948e-05, 2.4416240779645548e-05, 2.289511271036382e-05, 2.1468749859865355e-05, 2.013124852062731e-05, 1.88770727647114e-05, 1.7701031530981126e-05, 1.6598257146727136e-05, 1.5564185179339773e-05, 1.4594535547196585e-05, 1.368529480605396e-05, 1.283269954099886e-05, 1.2033220788021382e-05, 1.1283549431029272e-05, 1.0580582503694203e-05, 9.921410352164983e-06, 9.303304585595029e-06, 8.723706790059182e-06, 8.180217937026057e-06, 7.670588453079219e-06, 7.192708912251433e-06, 6.744601301900133e-06, 6.324410839031458e-06, 5.93039829110964e-06, 5.560932778037042e-06, 5.214485024440663e-06, 4.889621024739554e-06, 4.584996113221607e-06, 4.2993493949428085e-06, 4.0314985290113015e-06, 3.7803348338361076e-06, 3.5448186974651463e-06, 3.3239752756930585e-06, 3.116890457954824e-06, 2.922707080799114e-06, 2.740621383834352e-06, 2.5698796786155498e-06, 2.4097752331364575e-06, 2.2596453439494013e-06, 2.1188685945805474e-06, 1.9868622822549753e-06, 1.8630800064922681e-06, 1.747009408026301e-06, 1.6381700480572192e-06, 1.5361114178435997e-06, 1.4404110757482158e-06, 1.3506728990808625e-06, 1.2665254434107709e-06, 1.187620405351808e-06, 1.1136311799386789e-06, 1.0442515105957284e-06, 9.791942210402027e-07, 9.181900255672559e-07, 8.609864143860335e-07, 8.073466106761629e-07, 7.570485871521981e-07, 7.098841503516695e-07, 6.656580762154363e-07, 6.241873038437262e-07, 5.853001774358546e-07, 5.488357344152206e-07, 5.146430408498048e-07, 4.825805659525173e-07, 4.5251559588344037e-07, 4.243236855216992e-07, 3.978881426558445e-07, 3.7309954525888145e-07, 3.4985528740705263e-07, 3.280591565069102e-07, 3.0762093339298247e-07, 2.88456020181016e-07, 2.704850887713661e-07, 2.536337537772937e-07, 2.378322638829644e-07, 2.2301521362955157e-07, 2.091212716326396e-07, 1.9609292789546373e-07, 1.8387625577709343e-07, 1.724206872832923e-07, 1.6167880523276779e-07, 1.5160614741382972e-07, 1.421610202889667e-07, 1.33304328242545e-07, 1.249994119323361e-07, 1.1721189574487312e-07, 1.0990954479872528e-07, 1.0306213327204716e-07, 9.664131828124312e-08, 9.062052330754966e-08]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('c7791f19-5bb8-4182-9e1f-10a2a3690051');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2s66fHa43LDk"
      },
      "source": [
        "predict(X, model['W1'], model['W2'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}